{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe46b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84317510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n"
     ]
    }
   ],
   "source": [
    "df = df.sample(10000, random_state=2)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f36f2be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def preprocess(q):\n",
    "    \n",
    "    q = str(q).lower().strip()\n",
    "    \n",
    "    # Replace certain special characters with their string equivalents\n",
    "    q = q.replace('%', ' percent')\n",
    "    q = q.replace('$', ' dollar ')\n",
    "    q = q.replace('₹', ' rupee ')\n",
    "    q = q.replace('€', ' euro ')\n",
    "    q = q.replace('@', ' at ')\n",
    "    \n",
    "    # The pattern '[math]' appears around 900 times in the whole dataset.\n",
    "    q = q.replace('[math]', '')\n",
    "    \n",
    "    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n",
    "    q = q.replace(',000,000,000 ', 'b ')\n",
    "    q = q.replace(',000,000 ', 'm ')\n",
    "    q = q.replace(',000 ', 'k ')\n",
    "    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n",
    "    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n",
    "    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n",
    "    \n",
    "    # Decontracting words\n",
    "    # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "    # https://stackoverflow.com/a/19794953\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't've\": \"can not have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "    q_decontracted = []\n",
    "\n",
    "    for word in q.split():\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "\n",
    "        q_decontracted.append(word)\n",
    "\n",
    "    q = ' '.join(q_decontracted)\n",
    "    q = q.replace(\"'ve\", \" have\")\n",
    "    q = q.replace(\"n't\", \" not\")\n",
    "    q = q.replace(\"'re\", \" are\")\n",
    "    q = q.replace(\"'ll\", \" will\")\n",
    "    \n",
    "    # Removing HTML tags\n",
    "    q = BeautifulSoup(q)\n",
    "    q = q.get_text()\n",
    "    \n",
    "    # Remove punctuations\n",
    "    pattern = re.compile('\\W')\n",
    "    q = re.sub(pattern, ' ', q).strip()\n",
    "\n",
    "    \n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55531a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello bro  whats up in dollar'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"Hello bro! whats up in $\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a460e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python37\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df['question1'] = df['question1'].apply(preprocess)\n",
    "df['question2'] = df['question2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e34f973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>i am poor but i want to invest  what should i do</td>\n",
       "      <td>i am quite poor and i want to be very rich  wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>i am from india and live abroad  i met a guy f...</td>\n",
       "      <td>t i e t to thapar university to thapar univers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>why do so many people in the u s  hate the sou...</td>\n",
       "      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>consequences of bhopal gas tragedy</td>\n",
       "      <td>what was the reason behind the bhopal gas tragedy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "327711  327711  454161  454162   \n",
       "367788  367788  498109  491396   \n",
       "151235  151235  237843   50930   \n",
       "\n",
       "                                                question1  \\\n",
       "398782  what is the best marketing automation tool for...   \n",
       "115086   i am poor but i want to invest  what should i do   \n",
       "327711  i am from india and live abroad  i met a guy f...   \n",
       "367788  why do so many people in the u s  hate the sou...   \n",
       "151235                 consequences of bhopal gas tragedy   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "398782  what is the best marketing automation tool for...             1  \n",
       "115086  i am quite poor and i want to be very rich  wh...             0  \n",
       "327711  t i e t to thapar university to thapar univers...             0  \n",
       "367788  my boyfriend doesnt feel guilty when he hurts ...             0  \n",
       "151235  what was the reason behind the bhopal gas tragedy             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38286de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature engineering\n",
    "df['qsn1_len'] = df['question1'].str.len()\n",
    "df['qsn2_len'] = df['question2'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03646bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['qsn1_word_len'] = df['question1'].str.split().str.len()\n",
    "df['qsn2_word_len'] = df['question2'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f0ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_word(row):\n",
    "    q1 = set(map(lambda word: word.lower().strip(), row['question1'].split(' ')))\n",
    "    q2 = set(map(lambda word: word.lower().strip(), row['question2'].split(' ')))\n",
    "    return len(q1 & q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a1183b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giri', 'hello', 'iam', 'nlp', 'on', 'project', 'sunil', 'working'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(map(lambda word: word.lower().strip(), 'hello iam sunil giri working on NLP project'.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "223d2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['common_word'] = df.apply(common_word, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d9b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_word(row):\n",
    "    q1 = set(map(lambda word: word.lower().strip(), row['question1'].split(' ')))\n",
    "    q2 = set(map(lambda word: word.lower().strip(), row['question2'].split(' ')))\n",
    "    return (len(q1) + len(q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa89da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_word'] = df.apply(total_word, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8c2fc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>qsn1_len</th>\n",
       "      <th>qsn2_len</th>\n",
       "      <th>qsn1_word_len</th>\n",
       "      <th>qsn2_word_len</th>\n",
       "      <th>common_word</th>\n",
       "      <th>total_word</th>\n",
       "      <th>word_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>i am poor but i want to invest  what should i do</td>\n",
       "      <td>i am quite poor and i want to be very rich  wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "\n",
       "                                                question1  \\\n",
       "398782  what is the best marketing automation tool for...   \n",
       "115086   i am poor but i want to invest  what should i do   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "398782  what is the best marketing automation tool for...             1   \n",
       "115086  i am quite poor and i want to be very rich  wh...             0   \n",
       "\n",
       "        qsn1_len  qsn2_len  qsn1_word_len  qsn2_word_len  common_word  \\\n",
       "398782        75        76             13             13           12   \n",
       "115086        48        56             12             15            8   \n",
       "\n",
       "        total_word  word_share  \n",
       "398782          26        0.46  \n",
       "115086          24        0.33  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_share'] = round(df['common_word']/df['total_word'],2)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a873b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c96a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced features\n",
    "from nltk.corpus import stopwords \n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "def fetch_token_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    SAFE_DIV = 0.0001\n",
    "    \n",
    "    token_features = [0.0]*8\n",
    "    \n",
    "    #converting the sentence into tokens:\n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens)==0:\n",
    "        return token_features\n",
    "    \n",
    "    # Get the non-stopwords in question\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    # Get the stopword in questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    # Get the common non-stopwords form question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Get the common stopword from questions\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # Get the common token form question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    \n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    \n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # first word of both questions is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    \n",
    "    return token_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe8860a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features = df.apply(fetch_token_features, axis=1)\n",
    "\n",
    "df['cwc_min'] = list(map(lambda x:x[0], token_features))\n",
    "df['cwc_max'] = list(map(lambda x:x[1], token_features))\n",
    "df['csc_min'] = list(map(lambda x:x[2], token_features))\n",
    "df['csc_max'] = list(map(lambda x:x[3], token_features))\n",
    "df['ctc_min'] = list(map(lambda x:x[4], token_features))\n",
    "df['ctc_max'] = list(map(lambda x:x[5], token_features))\n",
    "df['last_word_eq'] = list(map(lambda x:x[6], token_features))\n",
    "df['first_word_eq'] = list(map(lambda x:x[7], token_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70732621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>qsn1_len</th>\n",
       "      <th>qsn2_len</th>\n",
       "      <th>qsn1_word_len</th>\n",
       "      <th>qsn2_word_len</th>\n",
       "      <th>...</th>\n",
       "      <th>total_word</th>\n",
       "      <th>word_share</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.874989</td>\n",
       "      <td>0.874989</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.923070</td>\n",
       "      <td>0.923070</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>i am poor but i want to invest  what should i do</td>\n",
       "      <td>i am quite poor and i want to be very rich  wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.714276</td>\n",
       "      <td>0.555549</td>\n",
       "      <td>0.583328</td>\n",
       "      <td>0.466664</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>i am from india and live abroad  i met a guy f...</td>\n",
       "      <td>t i e t to thapar university to thapar univers...</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>119</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428565</td>\n",
       "      <td>0.272725</td>\n",
       "      <td>0.149999</td>\n",
       "      <td>0.115384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>why do so many people in the u s  hate the sou...</td>\n",
       "      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>145</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>consequences of bhopal gas tragedy</td>\n",
       "      <td>what was the reason behind the bhopal gas tragedy</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.333330</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "327711  327711  454161  454162   \n",
       "367788  367788  498109  491396   \n",
       "151235  151235  237843   50930   \n",
       "\n",
       "                                                question1  \\\n",
       "398782  what is the best marketing automation tool for...   \n",
       "115086   i am poor but i want to invest  what should i do   \n",
       "327711  i am from india and live abroad  i met a guy f...   \n",
       "367788  why do so many people in the u s  hate the sou...   \n",
       "151235                 consequences of bhopal gas tragedy   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "398782  what is the best marketing automation tool for...             1   \n",
       "115086  i am quite poor and i want to be very rich  wh...             0   \n",
       "327711  t i e t to thapar university to thapar univers...             0   \n",
       "367788  my boyfriend doesnt feel guilty when he hurts ...             0   \n",
       "151235  what was the reason behind the bhopal gas tragedy             0   \n",
       "\n",
       "        qsn1_len  qsn2_len  qsn1_word_len  qsn2_word_len  ...  total_word  \\\n",
       "398782        75        76             13             13  ...          26   \n",
       "115086        48        56             12             15  ...          24   \n",
       "327711       104       119             26             20  ...          38   \n",
       "367788        58       145             13             30  ...          34   \n",
       "151235        34        49              5              9  ...          13   \n",
       "\n",
       "        word_share   cwc_min   cwc_max   csc_min   csc_max   ctc_min  \\\n",
       "398782        0.46  0.874989  0.874989  0.999980  0.999980  0.923070   \n",
       "115086        0.33  0.666644  0.666644  0.714276  0.555549  0.583328   \n",
       "327711        0.11  0.000000  0.000000  0.428565  0.272725  0.149999   \n",
       "367788        0.03  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "151235        0.23  0.749981  0.749981  0.000000  0.000000  0.599988   \n",
       "\n",
       "         ctc_max  last_word_eq  first_word_eq  \n",
       "398782  0.923070             1              1  \n",
       "115086  0.466664             1              1  \n",
       "327711  0.115384             0              0  \n",
       "367788  0.000000             0              0  \n",
       "151235  0.333330             1              0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de04306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance\n",
    "def fetch_length_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    SAFE_DIV = 0.0001\n",
    "    \n",
    "    length_features = [0.0]*8\n",
    "    \n",
    "    #converting the sentence into tokens:\n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens)==0:\n",
    "        return length_features\n",
    "    \n",
    "    # Absolute length feature\n",
    "    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    \n",
    "    # Average length feature\n",
    "    length_features[1] = (len(q1_tokens) + len(q2_tokens))\n",
    "    \n",
    "    strs = list(distance.lcsubstrings(q1, q2))\n",
    "    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n",
    "    \n",
    "    return length_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b24487",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_features = df.apply(fetch_length_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abs_len_diff'] = list(map(lambda x: x[0], length_features))\n",
    "df['mean_len'] = list(map(lambda x: x[1], length_features))\n",
    "df['longest_subs_ratio'] = list(map(lambda x: x[2], length_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5175eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c66c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy features\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fetch_fuzzy_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    fuzzy_features = [0.0]*4\n",
    "    \n",
    "    #fuzzy ratio\n",
    "    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n",
    "    \n",
    "    # fuzzy_partial_ratio\n",
    "    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n",
    "    \n",
    "    # TOken_sort_ratio\n",
    "    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n",
    "    \n",
    "    #token set ratio\n",
    "    fuzzy_features[3] = fuzz.token_sort_ratio(q1, q2)\n",
    "    \n",
    "    return fuzzy_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e900aec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_features = df.apply(fetch_fuzzy_features, axis=1)\n",
    "df['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\n",
    "df['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))\n",
    "df['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))\n",
    "df['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4109a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'is_duplicate']], hue='is_duplicate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['ctc_max', 'cwc_max', 'csc_max', 'is_duplicate']], hue='is_duplicate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591761b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['fuzz_ratio', 'fuzz_partial_ratio', 'token_sort_ratio', 'token_set_ratio', 'is_duplicate']], hue='is_duplicate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8960f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TSNE for dimensionality reduction for 15 features to 3 dimension\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = MinMaxScaler().fit_transform(df[['ctc_min','ctc_max', 'cwc_min', 'cwc_max', 'csc_min', 'csc_max','last_word_eq', 'first_word_eq', 'abs_len_diff' ,'mean_len', 'longest_subs_ratio', 'fuzz_ratio','fuzz_partial_ratio','token_sort_ratio','token_set_ratio']])\n",
    "y = df['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne2d = TSNE(n_components=2, init='random',\n",
    "#              random_state=101, method='barnes_hut',\n",
    "#              n_iter=1000,\n",
    "#              verbose=2,\n",
    "#              angle=0.5).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a45083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df[['question1', 'question2']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.drop(columns = ['id','qid1','qid2','question1','question2'])\n",
    "print(new_df.shape)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d4d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# nlp = spacy.load(\"C:/Python311/Lib/site-packages/en_core_web_lg/en_core_web_lg-3.5.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c5ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "question = list(temp_df['question1']) + list(temp_df['question2'])\n",
    "cv = CountVectorizer(max_features=2000)\n",
    "q1_arr, q2_arr = np.vsplit(cv.fit_transform(question).toarray(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df1 = pd.DataFrame(q1_arr,index=temp_df.index)\n",
    "temp_df2 = pd.DataFrame(q2_arr,index=temp_df.index)\n",
    "temp_new = pd.concat([temp_df1, temp_df2], axis=1)\n",
    "temp_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([new_df, temp_new], axis=1)\n",
    "print(final_df.shape)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.drop(['question1', 'question2'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105538c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_df.iloc[:,1:].values,final_df.iloc[:,0].values,test_size=0.2,random_state=24)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ccccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## We need to change my X_train and X_test to 2d\n",
    "# X_train_2d = np.stack(X_train)\n",
    "# X_test_2d = np.stack(X_test)\n",
    "# X_train_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6eee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train_2d.shape)\n",
    "# print(X_test_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a055d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd9a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48214f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xg = XGBClassifier()\n",
    "xg.fit(X_train, y_train)\n",
    "y_pred1 = xg.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a19841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pre = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense,Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from keras.layers import Reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee84fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# define the model architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(4022,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Reshape((1, 64)))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# set up early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c61576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_common_words(q1,q2):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))    \n",
    "    return len(w1 & w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa899bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_total_words(q1,q2):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))    \n",
    "    return (len(w1) + len(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f15ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fetch_token_features(q1,q2):\n",
    "    \n",
    "    SAFE_DIV = 0.0001 \n",
    "\n",
    "    STOP_WORDS = stopwords.words(\"english\")\n",
    "    \n",
    "    token_features = [0.0]*8\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    # Get the non-stopwords in Questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    #Get the stopwords in Questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    # Get the common non-stopwords from Question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Get the common stopwords from Question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # Get the common Tokens from Question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    \n",
    "    \n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    \n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    \n",
    "    return token_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17abe786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fetch_length_features(q1,q2):\n",
    "    \n",
    "    length_features = [0.0]*3\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return length_features\n",
    "    \n",
    "    # Absolute length features\n",
    "    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    \n",
    "    #Average Token Length of both Questions\n",
    "    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "    \n",
    "    strs = list(distance.lcsubstrings(q1, q2))\n",
    "    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n",
    "    \n",
    "    return length_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db89bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fetch_fuzzy_features(q1,q2):\n",
    "    \n",
    "    fuzzy_features = [0.0]*4\n",
    "    \n",
    "    # fuzz_ratio\n",
    "    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n",
    "\n",
    "    # fuzz_partial_ratio\n",
    "    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n",
    "\n",
    "    # token_sort_ratio\n",
    "    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n",
    "\n",
    "    # token_set_ratio\n",
    "    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n",
    "\n",
    "    return fuzzy_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_point_creator(q1,q2):\n",
    "    \n",
    "    input_query = []\n",
    "    \n",
    "    # preprocess\n",
    "    q1 = preprocess(q1)\n",
    "    q2 = preprocess(q2)\n",
    "    \n",
    "    # fetch basic features\n",
    "    input_query.append(len(q1))\n",
    "    input_query.append(len(q2))\n",
    "    \n",
    "    input_query.append(len(q1.split(\" \")))\n",
    "    input_query.append(len(q2.split(\" \")))\n",
    "    \n",
    "    input_query.append(test_common_words(q1,q2))\n",
    "    input_query.append(test_total_words(q1,q2))\n",
    "    input_query.append(round(test_common_words(q1,q2)/test_total_words(q1,q2),2))\n",
    "    \n",
    "    # fetch token features\n",
    "    token_features = test_fetch_token_features(q1,q2)\n",
    "    input_query.extend(token_features)\n",
    "    \n",
    "    # fetch length based features\n",
    "    length_features = test_fetch_length_features(q1,q2)\n",
    "    input_query.extend(length_features)\n",
    "    \n",
    "    # fetch fuzzy features\n",
    "    fuzzy_features = test_fetch_fuzzy_features(q1,q2)\n",
    "    input_query.extend(fuzzy_features)\n",
    "    \n",
    "    # bow feature for q1\n",
    "    q1_bow = cv.transform([q1]).toarray()\n",
    "    \n",
    "    # bow feature for q2\n",
    "    q2_bow = cv.transform([q2]).toarray()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return np.hstack((np.array(input_query).reshape(1,22),q1_bow,q2_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = 'Where is the capital of India?'\n",
    "q2 = 'What is the current capital of Pakistan?'\n",
    "q3 = 'Which city serves as the capital of India?'\n",
    "q4 = 'What is the business capital of India?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict(query_point_creator(q1,q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3cdadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60626b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle.dump(rf,open('model.pkl','wb'))\n",
    "# pickle.dump(cv,open('cv.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75177dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(fetch_token_features,open('stopwords.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a861ae0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
